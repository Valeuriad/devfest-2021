# DevFest Nantes 2021 - IArt ou comment apprendre à une machine à tagger et à composer


Ce repository GIT contient le code utilisé pour la conférence **IArt ou comment apprendre à une machine à tagger et à composer** présentée au DevFest de Nantes 2021.  

Abstract :  
*This summary was generated by the Turing-NLG language model itself, c’est ainsi que se termine la présentation du modèle de langage Turing-NLG, réseau de neurones de 17 milliards de paramètres sorti des labos de Microsoft.
Alors que les modèles de machine learning (souvent raccourcis en IA) deviennent capables de générer du contenu pouvant rivaliser avec le test de Turing, nous allons nous pencher sur les débouchées artistiques de ces avancées récentes.
Dans cette présentation, nous nous intéresserons aux approches de Neural Style Transfer (ie: la copie du style d’un artiste sur une nouvelle image) et de génération automatique de musique.
Pour ces deux applications, nous allons explorer et vulgariser les rouages théoriques des algorithmes à l'état de l’art (les modèles de deep learning, et une initiation au traitement d’image et de signal) et les mettre en application pour 1) générer automatiquement un tag/graffiti à partir d’un texte libre et 2) générer automatiquement un morceau de hip-hop.*


Le projet est découpé en 4 briques :

- **Style Transfer** : Introduction au Neural Style Transfer & création de tags/graffitis
- **Text Generation** : Introduction à la génération de texte & création de paroles de Hip-Hop
- **Beat Generation** : Introduction à la génération d'un signal audio & création d'un beat de Hip-Hop
- **Speech Synthesis** : Synthèse vocale d'un texte & création d'un clip vidéo avec les éléments des 3 autres briques


Au final, on est capable de générer un clip vidéo avec des données entièrement générées par l'IA (images, textes & sons).


---


### Installation

Chaque brique consiste en un notebook python, qui nous semblait être le format le plus approprié pour une initiation à la génération de contenus avec l'IA.  

Pour utiliser ce projet, il vous faut :

0. (Optionnel, mais conseillé) Créer et utiliser un environnement virtuel :
  ```bash
  pip install virtualenv
  python -m venv venv_devfest  # Installation du venv
  # Activation du venv selon votre OS
  source venv_devfest/bin/activate  # UNIX
  venv_devfest\Scripts\activate  # WINDOWS
  ```

1. Installer les requirements :
  ```bash
  pip install -r requirements.txt
  ```

2. Télécharger et ajouter FFMPEG à votre PATH :  

  Lien windwows : https://github.com/BtbN/FFmpeg-Builds/releases/download/autobuild-2021-10-02-12-23/ffmpeg-N-103927-g68815d6791-win64-gpl.zip

3. Pour pouvoir utiliser Tensorflow GPU il est nécessaire d'installer CuDA et CuDNN. L'environnement testé :
- Python 3.7.5
- Tensorflow 2.3.1
- Microsoft Visual C++ 2019
- CuDA Version 10.1.243 (10.1 update 2 - August 2019)
- CuDNN cudnn-10.1-windows10-x64-v7.6.4.38
---


### Utilisation

0. (Optionnel, mais conseillé) Ajouter votre environnement virtuel à jupyter :
  ```bash
  pip install ipykernel
  python -m ipykernel install --name=venv_devfest
  ```

1. Lancer simplement un notebook jupyter à la racine du projet :
  ```bash
  jupyter notebook
  ```

2. Naviguer dans les répertoires, et lancer les fichier .ipynb via jupyter. Enjoy !


---


### Mainteners

Nicolas GREFFARD : greffard.nicolas@valeuriad.fr  
Alexandre GAREL : garel.alexandre@valeuriad.fr
